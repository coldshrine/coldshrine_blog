---
title: AI Engineering by Chip Huyen Applied to Data Architecture Building
description: A fun dive into how data engineering meets AI through RAGs
date: "2024-06-10"
url: https://learning.oreilly.com/library/view/ai-engineering/9781098166298/
published: true
---

# RAGs Overview

Welcome to the wonderful world of **Retrieval-Augmented Generation (RAG)**! Think of RAG as the superhero of AI—it enhances model performance by retrieving only the most relevant data before generating a response. This nifty trick makes the process faster and more efficient, especially when you're juggling large datasets. Because who likes waiting? 😅

## How RAG Works

Imagine you're looking for answers in a library full of books (a.k.a. your data). Instead of reading every single book (yawn!), a RAG system retrieves only the relevant ones. For instance, in *Reading Wikipedia to Answer Open-Domain Questions* (Chen et al., 2017), a model retrieves five relevant Wikipedia pages for a query to generate a better answer. Less time searching, more time solving problems! 🦸‍♂️

[![](/2_1_retreive_then_generate.png)](https://arxiv.org/abs/1704.00051)

## RAG Architecture and Components

A RAG system consists of two key components:

1. A **retriever** that pulls relevant information from external memory sources.
2. A **generator** that creates responses based on the retrieved information.

As data engineers, we play a crucial role in designing and maintaining the architecture of these systems. This involves handling large-scale data indexing and querying efficiently—because no one likes a slow database! 🐢💨

### The Retriever's Superpowers

The retriever has two main functions: **indexing** and **querying**. Here’s where our data engineering expertise comes into play:

- **Indexing**: This is like organizing your messy closet. You process and structure data, breaking documents into smaller, manageable chunks. This helps prevent overwhelming the model with overly long contexts, making it more efficient. Think of it as decluttering for your data! 🧹

- **Querying**: This function retrieves those chunks in response to a specific user query. As data engineers, we ensure the system can scale to handle high data volumes, optimize search algorithms, and minimize latency. Post-processing the retrieved chunks to merge them into a usable format is another key step, preparing the input for the generative model.

[![](/2_2_high_level_RAG.png)](https://learning.oreilly.com/library/view/ai-engineering/9781098166298/ch06.html#6_rag_overview_1726600321985229)

In modern systems, retrievers and generators are often trained separately. However, an end-to-end approach, including retraining the retriever for specific data sets, can improve performance. For us data engineers, fine-tuning and scaling these components is vital to the success of a RAG system.

# Retrieval Algorithms

Retrieval isn’t just a RAG thing. It's the backbone of search engines, recommender systems, log analytics, and more. 

## Retrieval vs. Search

Retrieval is typically limited to one database or system, while search involves retrieval across various systems. For our purposes, we’ll use retrieval and search interchangeably. Just keep that in your back pocket for trivia night! 🧠

## Sparse vs. Dense Retrieval

Retrieval algorithms can be divided into **sparse** and **dense** retrieval.

- **Sparse Retrievers**: These represent data using sparse vectors, where most values are 0. Term-based retrieval is considered sparse. Think of it as having a few snacks in a big empty pantry—only a little bit of useful data. 🍿

- **Dense Retrievers**: These represent data using dense vectors, where most values are non-zero. Embedding-based retrieval typically falls into this category. Imagine a full pantry bursting with goodies—lots of useful information! 🍰

## Term-Based Retrieval

The most straightforward way to find relevant documents is by keywords, also known as **lexical retrieval**. For instance, if your query is "AI engineering," you’ll retrieve all documents containing that term. However, there are two main issues with this approach:

1. You might get swamped with too many documents (good luck finding the needle in that haystack! 🐴).
2. Important terms can get overshadowed by less informative ones (like how that one friend always talks over everyone else at dinner). 🙄

### TF-IDF

Enter TF-IDF—a fancy way to rank documents using term frequency (TF) and inverse document frequency (IDF). It helps ensure that the most relevant documents rise to the top. The formula is:

## TF-IDF Equation

The TF-IDF equation can be represented as:

TF-IDF(D, Q) = Σ (f(t, D) * IDF(t)) for all t in Q

Two popular term-based retrieval solutions are **Elasticsearch** and **BM25**.

### Elasticsearch Example

Elasticsearch, built on top of Lucene, uses an inverted index to allow fast retrieval of documents. Here's a simplified example of an inverted index that even your pet goldfish would understand:

| Term    | Document Count | (Document Index, Term Frequency) |
|---------|----------------|-----------------------------------|
| banana  | 2              | (10, 3), (5, 2)                   |
| machine | 4              | (1, 5), (10, 1), (38, 9), (42, 5) |
| learning | 3              | (1, 5), (38, 7), (42, 5)         |

### BM25

BM25 is a snazzy upgrade to TF-IDF that normalizes term frequency by document length, making it a robust baseline for comparing modern retrieval algorithms. It’s like getting the perfect fit for your favorite jeans—comfortable and just right! 👖✨

## Embedding-Based Retrieval

Embedding-based retrieval ranks documents based on semantic similarity rather than lexical similarity. This means you’re finding documents that *mean* the same thing, even if they don’t use the exact words.

[![](/2_3_embedding_based_retriever.png)](https://medium.com/better-ml/embedding-learning-for-retrieval-29af1c9a1e65)

Here's how it works:

1. **Indexing**: Convert original data into embeddings stored in a vector database. It’s like translating a book into emojis—fun and engaging! 📚➡️😀

2. **Querying**: Convert the query into an embedding and fetch the closest data chunks. Imagine asking your friend for their favorite pizza toppings and getting all the toppings that match your vibe! 🍕✨

### How It Works

- **Convert the Query**: Use the same embedding model that was used during indexing.
- **Fetch k Closest Chunks**: Determine how many data chunks (k) to fetch based on the use case and query. It’s like picking the best slices from a pizza—everyone loves a good selection! 🍕

### Vector Search

Vector search is framed as a nearest-neighbor search problem. Given a query, find the k nearest vectors using metrics like cosine similarity. It's like searching for your favorite playlist among a million songs—your data should jam, not just sit there! 🎶

#### Popular Libraries

Some popular libraries for vector search include:

- **FAISS** (Facebook AI Similarity Search)
- **Google's ScaNN**
- **Spotify’s ANNOY**
- **hnswlib** (Hierarchical Navigable Small World)

# The Future of AI Engineering in Data Architecture

As AI evolves, so do the techniques we use to build and maintain data architectures. **Retrieval-Augmented Generation** is just one example of how intelligent retrieval systems can vastly improve data processing and response generation. Data engineers are central to this revolution, ensuring that the architecture supports efficient retrieval and generation.

The future might even involve **agents**—autonomous systems that can query, retrieve, and generate data without human intervention. This opens exciting possibilities in automation, from handling complex datasets to real-time decision-making systems. Just imagine a world where data engineers design frameworks that let AI "agents" manage large-scale infrastructure independently—now that’s next-level data architecture!
